# Understanding Metrics in TranscribeSight

This document explains the various metrics used in TranscribeSight for evaluating speech transcription accuracy.

## Traditional Accuracy Metrics

### Word Error Rate (WER)
- **Definition**: The percentage of words that were incorrectly transcribed.
- **Formula**: WER = (S + D + I) / N, where S = substitutions, D = deletions, I = insertions, N = number of words in reference.
- **Interpretation**: Lower is better. A WER of 0% means perfect transcription, while a WER of 100% means complete failure.
- **Use case**: General purpose accuracy measurement, most commonly used metric in STT evaluation.

### Character Error Rate (CER)
- **Definition**: The percentage of characters that were incorrectly transcribed.
- **Formula**: CER = (S + D + I) / N, where S = substitutions, D = deletions, I = insertions, N = number of characters in reference.
- **Interpretation**: Lower is better. More fine-grained than WER.
- **Use case**: Useful for languages where characters carry more meaning than words, or when partial word accuracy is important.

### Token Error Rate (TER)
- **Definition**: The percentage of tokens (words and punctuation) that were incorrectly transcribed.
- **Formula**: Similar to WER but applied to tokenized text.
- **Interpretation**: Lower is better. Accounts for punctuation better than WER.
- **Use case**: Useful when punctuation accuracy is important, such as in legal or medical transcription.

### Match Error Rate (MER)
- **Definition**: The proportion of all errors (substitutions, deletions, insertions) to the total alignment length.
- **Formula**: MER = (S + D + I) / (H + S + D + I), where H = correct matches.
- **Interpretation**: Lower is better. More sensitive to short reference texts than WER.
- **Use case**: Provides a balanced view that considers the entire alignment.

### Word Information Lost (WIL)
- **Definition**: The amount of information lost during transcription, considering both errors and length differences.
- **Formula**: WIL = 1 - (1 - WER) * (1 - |ref_len - hyp_len| / ref_len)
- **Interpretation**: Lower is better. Accounts for both accuracy and completeness.
- **Use case**: Useful for evaluating information retention rather than just mechanical accuracy.

### Word Information Preserved (WIP)
- **Definition**: The amount of information preserved during transcription.
- **Formula**: WIP = 1 - WIL
- **Interpretation**: Higher is better. Direct inverse of WIL.
- **Use case**: Positive framing of information retention in transcription.

## Semantic Metrics

### SeMaScore
- **Definition**: Semantic similarity score based on sentence embeddings.
- **Implementation**: Uses cosine similarity between sentence embeddings generated by a language model.
- **Interpretation**: Higher is better (0-100). Measures meaning preservation rather than exact wording.
- **Use case**: Evaluating how well the transcription preserves the meaning of the speech.

### STS (Semantic Textual Similarity)
- **Definition**: Measures the semantic similarity between reference and predicted text based on sentences.
- **Implementation**: Calculates similarity between sentence embeddings using cosine distance.
- **Interpretation**: Higher is better (0-100). Focuses on sentence-level meaning.
- **Use case**: Evaluating semantic similarity for conversational or contextual speech.

### BERTScore
- **Definition**: Measures semantic similarity using contextual embeddings.
- **Implementation**: Computes token-level matches using BERT embeddings.
- **Interpretation**: Higher is better (0-100). More robust to paraphrasing than lexical metrics.
- **Use case**: Evaluating semantic similarity with better handling of context and word order.

### LLM-based Meaning Preservation
- **Definition**: Uses a large language model (GPT-4o mini) to evaluate meaning preservation.
- **Implementation**: Prompts the LLM to rate semantic similarity on a scale of 0-100.
- **Interpretation**: Higher is better. Provides a human-like judgment of meaning preservation.
- **Use case**: Getting an assessment similar to how a human would judge meaning preservation.

### SWWER (Semantic-Weighted Word Error Rate)
- **Definition**: Word error rate that weighs errors by semantic importance.
- **Implementation**: Assigns higher weight to content words and lower weight to stop words.
- **Interpretation**: Lower is better. Combines lexical accuracy with semantic importance.
- **Use case**: Prioritizing errors in meaningful words over errors in less important words.

## Performance Metrics

### Processing Time
- **Definition**: The time taken to process and transcribe the audio.
- **Measurement**: Seconds from start to completion of transcription.
- **Interpretation**: Lower is better. Indicates transcription speed.
- **Use case**: Evaluating services for real-time or time-sensitive applications.

### Cost
- **Definition**: The estimated cost of transcription based on audio duration.
- **Calculation**: Based on published API pricing for each service.
- **Interpretation**: Lower is better. Direct financial metric.
- **Use case**: Budget planning and cost optimization.

### Efficiency Score
- **Definition**: A balanced metric considering both cost and processing time.
- **Formula**: Efficiency Score = 100 / (1 + Cost-Time Index), where Cost-Time Index is the geometric mean of cost and time.
- **Interpretation**: Higher is better. Balances speed and cost.
- **Use case**: Finding the most efficient service for specific needs.

## Additional Resources

For more information on transcription metrics:
- [Speech Recognition Metrics on Wikipedia](https://en.wikipedia.org/wiki/Word_error_rate)
- [NIST Speech Recognition Scoring Toolkit](https://www.nist.gov/itl/iad/mig/tools)
- [BERTScore: Evaluating Text Generation with BERT](https://arxiv.org/abs/1904.09675)
- [Semantic Textual Similarity](https://www.aclweb.org/anthology/S17-2001/)